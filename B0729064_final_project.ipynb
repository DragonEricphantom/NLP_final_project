{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B0729064_final_project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.7.7 64-bit"},"language_info":{"name":"python","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","interpreter":{"hash":"9164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MFb55c14OQK","executionInfo":{"status":"ok","timestamp":1629903503886,"user_tz":-480,"elapsed":42551,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"a43c8309-ff1d-4d67-9b84-6091004f2099"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_pVuRSHjAGPr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"91e936a1-0dfe-4074-82be-8f77075b983f"},"source":["import numpy as np \n","import pandas as pd \n","import os\n","import jieba\n","import jieba.analyse\n","import re\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split  \n","\n","def cuttrain_data(df):\n","  cuttitle = []\n","  for d in df['title'].astype(str):\n","    cut = jieba.analyse.extract_tags(d,topK=2)\n","    cuttitle.append(cut)\n","  print(\"in cuttitle\")\n","  # for d in cuttitle:\n","  #   print(d)\n","  cutcontext = []\n","  for d in df['context']:\n","    cut = jieba.analyse.extract_tags(d,topK=2)\n","    cutcontext.append(cut)\n","  # print(\"in context\")\n","  # for d in cutcontext:\n","  #   print(d)\n","  push = []\n","  for d in df['push']:\n","    push.append(d)\n","  down = []\n","  for d in df['down']:\n","    down.append(d)\n","\n","  data = {'title':cuttitle ,'context':cutcontext ,'push':push ,'down':down}\n","  df1 = pd.DataFrame(data)\n","\n","  df1['context'] = df1['context'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","  # print(\"df1['context'] = \")\n","  # print(df1['context'])\n","  df1['title'] = df1['title'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","  # print(\"df1['title'] = \")\n","  # print(df1['title'])\n","  df1['push_down'] = df1['title'].astype(str)+' '+df1['context'].astype(str)+' '+df1['push'].astype(str)+' '+df1['down'].astype(str)\n","\n","  return df1\n","\n","def cuttest_data(df):\n","  cuttitle = []\n","  for d in df['title'].astype(str):\n","    cut = jieba.analyse.extract_tags(d,topK=2)\n","    cuttitle.append(cut)\n","  cutcontext = []\n","  for d in df['context']:\n","    cut = jieba.analyse.extract_tags(d,topK=10)\n","    cutcontext.append(cut)\n","\n","  data = {'title':cuttitle ,'context':cutcontext}\n","  df1 = pd.DataFrame(data)\n","\n","  df1['context'] = df1['context'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","  df1['title'] = df1['title'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","  df1['key'] = df1['title'].astype(str)+' '+df1['context'].astype(str)\n","  return df1\n","\n","def count(df,df2):\n","  vectorizer = CountVectorizer()\n","  X = vectorizer.fit_transform(df['push_down'])\n","  X2 = vectorizer.transform(df2['key'])\n","  tfidf = TfidfTransformer() \n","  tf = tfidf.fit_transform(X)\n","  tf2 = tfidf.transform(X2)\n","  return tf ,tf2\n","\n","def knn_push(tf_train,train,tf_test):\n","  X_train, X_test, y_train, y_test = train_test_split(tf_train, train['push'].astype(str), test_size=0.2, random_state=1)\n","  clf=KNeighborsClassifier(n_neighbors=70, p=2)\n","  clf.fit(X_train,y_train)\n","  y_pred = clf.predict(X_test)\n","  ans_push = y_pred\n","  test_p = clf.predict(tf_test)\n","  # print(len(test_p))\n","  return test_p\n","\n","def knn_down(tf_train,train,tf_test):\n","  X_train, X_test, y_train, y_test = train_test_split(tf_train, train['down'].astype(str), test_size=0.2, random_state=1)\n","  clf=KNeighborsClassifier(n_neighbors=70, p=2)\n","  clf.fit(X_train,y_train)\n","  y_pred = clf.predict(X_test)\n","  ans_push = y_pred \n","  test_d = clf.predict(tf_test)\n","  print(len(test_d))\n","  return test_d\n","\n","\n","\n","test_index=[]\n","def save(test_index,test_p,test_d,test):\n","  for i in range(len(test)):\n","    test_index.append(i)\n","  data = {'index': test_index, 'push': test_p, 'down': test_d}\n","  export = pd.DataFrame(data)\n","  export.to_csv('projectfinal.csv' ,index = False)\n","\n","\n","\n","if __name__ == '__main__':\n","  train_data = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train_data.csv\")\n","  train_label = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train_label.csv\")\n","  test_data = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/test_data.csv\")\n","  files = glob('/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train*.csv')\n","  df_list = [pd.read_csv(file) for file in files]  \n","  total_train_data = pd.merge(df_list[0], df_list[1], on='index')\n","\n","  the_training_data = total_train_data[['title' ,'context' ,'push' ,'down']]\n","  the_test_data = test_data[['title' ,'context']]\n","\n","  train = cuttrain_data(the_training_data)\n","\n","  test = cuttest_data(the_test_data) #31849\n","  # print(\"len(test)\")\n","  # print(len(test))\n","\n","  tf_train , tf_test = count(train,test)\n","  # tf_test = count_test(test,vector,tfidf)\n","  # print(\"len(tf_train)\")\n","  # print(tf_train.shape)\n","\n","\n","  test_p = knn_push(tf_push,train,tf_test)\n","\n","  test_d = knn_down(tf_down,train,tf_test)\n","\n","  save(test_index,test_p,test_d,test)\n","\n","  my_answer= pd.read_csv(\"projectfinal.csv\")\n","  print(\"ans\")\n","  my_answer\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["in cuttitle\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6UFMwarkEc4K"},"source":[""],"execution_count":null,"outputs":[]}]}