{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B0729064_final_project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.7.7 64-bit"},"language_info":{"name":"python","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","interpreter":{"hash":"9164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MFb55c14OQK","executionInfo":{"status":"ok","timestamp":1629903503886,"user_tz":-480,"elapsed":42551,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"a43c8309-ff1d-4d67-9b84-6091004f2099"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_pVuRSHjAGPr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"08263339-1231-4827-df46-07293cd2e71e"},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","from glob import glob\n","import jieba\n","import jieba.analyse\n","\n","import re\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split  \n","\n","def cuttrain_data(df):\n","  cuttitle = []\n","  for d in df['title'].astype(str):\n","    cut = jieba.analyse.extract_tags(d,topK=2)\n","    cuttitle.append(cut)\n","  print(\"in cuttitle\")\n","  # for d in cuttitle:\n","  #   print(d)\n","  cutcontext = []\n","  for d in df['context']:\n","    cut = jieba.analyse.extract_tags(d,topK=2)\n","    cutcontext.append(cut)\n","  # print(\"in context\")\n","  # for d in cutcontext:\n","  #   print(d)\n","  push = []\n","  for d in df['push']:\n","    push.append(d)\n","  down = []\n","  for d in df['down']:\n","    down.append(d)\n","\n","  data = {'title':cuttitle ,'context':cutcontext ,'push':push ,'down':down}\n","  df1 = pd.DataFrame(data)\n","\n","  df1['context'] = df1['context'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","  # print(\"df1['context'] = \")\n","  # print(df1['context'])\n","  df1['title'] = df1['title'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","  # print(\"df1['title'] = \")\n","  # print(df1['title'])\n","  df1['ans_push'] = df1['title'].astype(str)+' '+df1['context'].astype(str)+' '+df1['push'].astype(str)\n","  # print(\"df1['ans_push'] = \")\n","  # for d in df1['ans_push']:\n","  #   print(d)\n","  df1['ans_down'] = df1['title'].astype(str)+' '+df1['context'].astype(str)+' '+df1['down'].astype(str)\n","  # print(\"df1['ans_down'] = \")\n","  # for d in df1['ans_down']:\n","  #   print(d)\n","  return df1\n","\n","def cuttest_data(df):\n","  cuttitle = []\n","  for d in df['title'].astype(str):\n","    cut = jieba.analyse.extract_tags(d,topK=2)\n","    cuttitle.append(cut)\n","  cutcontext = []\n","  for d in df['context']:\n","    cut = jieba.analyse.extract_tags(d,topK=10)\n","    cutcontext.append(cut)\n","\n","  data = {'title':cuttitle ,'context':cutcontext}\n","  df1 = pd.DataFrame(data)\n","\n","  df1['context'] = df1['context'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","  df1['title'] = df1['title'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","  return df1\n","\n","def count_push(df):\n","  vectorizer = CountVectorizer()\n","  X = vectorizer.fit_transform(df['ans_push'])\n","  tfidf = TfidfTransformer() \n","  tf=tfidf.fit_transform(X)\n","  word = vectorizer.get_feature_names() #詞袋 \n","  return tf\n","\n","def count_down(df):\n","  vectorizer = CountVectorizer()\n","  X = vectorizer.fit_transform(df['ans_down'])\n","  tfidf = TfidfTransformer() \n","  tf=tfidf.fit_transform(X)\n","  word = vectorizer.get_feature_names() #詞袋 \n","  return tf\n","\n","def knn_push(x_data,y_data):\n","  X_train, X_test, y_train, y_test = train_test_split(x_data, y_data['push'].astype(str), random_state=2)\n","  clf=KNeighborsClassifier(n_neighbors=5)\n","  clf.fit(X_train,y_train)\n","  y_pred = clf.predict(X_test)\n","  ans_push = y_pred    # 預測模型\n","  print(\"y_pred\")\n","  print(y_pred)\n","  \n","  y_test=y_test.values\n","  # print(\"精準度：\",metrics.accuracy_score(y_test, y_pred))\n","  return ans_push\n","def knn_down(x_data,y_data):\n","  X_train, X_test, y_train, y_test = train_test_split(x_data, y_data['down'].astype(str), test_size=0.2, random_state=2)\n","  clf=KNeighborsClassifier(n_neighbors=5)\n","  clf.fit(X_train,y_train)\n","  y_pred = clf.predict(X_test)    # 預測模型\n","  print(\"y_pred\")\n","  ans_down = y_pred\n"," \n","  y_test=y_test.values\n","  # print(\"精準度：\",metrics.accuracy_score(y_test, y_pred))\n","  return ans_down\n","\n","train_data = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train_data.csv\")\n","train_label = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train_label.csv\")\n","test_data = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/test_data.csv\")\n","files = glob('/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train*.csv')\n","df_list = [pd.read_csv(file) for file in files]  #串列中包含兩個Pandas DataFrame\n","total_train_data = pd.merge(df_list[0], df_list[1], on='index')\n","\n","\n","the_training_data = total_train_data[['title' ,'context' ,'push' ,'down']]\n","the_test_data = test_data[['title' ,'context']]\n","\n","\n","train = cuttrain_data(the_training_data)\n","\n","test = cuttest_data(the_test_data)\n","\n","tf_push = count_push(train)\n","tf_down = count_down(train)\n","\n","# knn(tf_push,train,test)\n","# knn(tf_down,train,test)\n","knn_push(tf_push,train)\n","knn_down(tf_down,train)\n","\n","ans_push = knn_push(tf_push,train)\n","for i in ans_push:\n","  print(i)\n","ans_down = knn_down(tf_down,train)\n","for i in ans_down:\n","  print(i)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["in cuttitle\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6UFMwarkEc4K"},"source":[""],"execution_count":null,"outputs":[]}]}