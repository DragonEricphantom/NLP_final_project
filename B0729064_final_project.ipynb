{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B0729064_final_project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.7.7 64-bit"},"language_info":{"name":"python","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","interpreter":{"hash":"9164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MFb55c14OQK","executionInfo":{"status":"ok","timestamp":1629894415044,"user_tz":-480,"elapsed":41393,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"b863d24c-04c3-4514-b985-843a760e4d3f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_pVuRSHjAGPr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629897410996,"user_tz":-480,"elapsed":43637,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"f13dd60a-9299-43b4-bf79-5e4c27de5c72"},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","from glob import glob\n","import jieba\n","import jieba.analyse\n","\n","import re\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split  \n","\n","def cuttrain_data(df):\n","  cuttitle = []\n","  for d in df['title'].astype(str):\n","    cut = jieba.analyse.extract_tags(d,topK=2)\n","    cuttitle.append(cut)\n","  print(\"in cuttitle\")\n","  # for d in cuttitle:\n","  #   print(d)\n","  cutcontext = []\n","  for d in df['context']:\n","    cut = jieba.analyse.extract_tags(d,topK=2)\n","    cutcontext.append(cut)\n","  # print(\"in context\")\n","  # for d in cutcontext:\n","  #   print(d)\n","  push = []\n","  for d in df['push']:\n","    push.append(d)\n","  down = []\n","  for d in df['down']:\n","    down.append(d)\n","\n","  data = {'title':cuttitle ,'context':cutcontext ,'push':push ,'down':down}\n","  df1 = pd.DataFrame(data)\n","\n","  df1['context'] = df1['context'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","  print(\"df1['context'] = \")\n","  print(df1['context'])\n","  df1['title'] = df1['title'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","  print(\"df1['title'] = \")\n","  print(df1['title'])\n","  df1['ans_push'] = df1['title'].astype(str)+' '+df1['context'].astype(str)+' '+df1['push'].astype(str)\n","  df1['ans_down'] = df1['title'].astype(str)+' '+df1['context'].astype(str)+' '+df1['down'].astype(str)\n","  return df1\n","\n","def cuttest_data(df):\n","  cuttitle = []\n","  for d in df['title'].astype(str):\n","    cut = jieba.analyse.extract_tags(d,topK=2)\n","    cuttitle.append(cut)\n","  cutcontext = []\n","  for d in df['context']:\n","    cut = jieba.analyse.extract_tags(d,topK=10)\n","    cutcontext.append(cut)\n","\n","def count(df):\n","  vectorizer = CountVectorizer()\n","  X = vectorizer.fit_transform(df['ans_push'])\n","  tfidf = TfidfTransformer() \n","  tf=tfidf.fit_transform(X)\n","  word = vectorizer.get_feature_names() #詞袋 \n","  print(\"tf = \")\n","  print(tf)\n","  return tf\n","\n","def count(df):\n","  vectorizer = CountVectorizer()\n","  X = vectorizer.fit_transform(df['ans_down'])\n","  tfidf = TfidfTransformer() \n","  tf=tfidf.fit_transform(X)\n","  word = vectorizer.get_feature_names() #詞袋 \n","  print(\"tf = \")\n","  print(tf)\n","  return tf\n","\n","def knn(X,Y):\n","  X_train, X_test, y_train, y_test = train_test_split(X, Y['type'].str[0:2], test_size=0.07)\n","  clf=KNeighborsClassifier(n_neighbors=51)\n","  clf.fit(X_train,y_train)\n","\n","  y_pred = clf.predict(X_test)    # 預測模型 \n","  y_test=y_test.values\n","  print(\"精準度：\",metrics.accuracy_score(y_test, y_pred))\n","\n","\n","\n","\n","\n","train_data = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train_data.csv\")\n","train_label = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train_label.csv\")\n","test_data = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/test_data.csv\")\n","files = glob('/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train*.csv')\n","df_list = [pd.read_csv(file) for file in files]  #串列中包含兩個Pandas DataFrame\n","total_train_data = pd.merge(df_list[0], df_list[1], on='index')\n","\n","\n","the_training_data = total_train_data[['title' ,'context' ,'push' ,'down']]\n","the_test_data = test_data[['title' ,'context']]\n","the_test_data\n","\n","ls = cuttrain_data(the_training_data)\n","\n"],"execution_count":37,"outputs":[{"output_type":"stream","text":["in cuttitle\n","in context\n","title\n","context\n","push\n","down\n","df1['context'] = \n","0           資料 自殺\n","1           舉起 甩奶\n","2           問卦 張貼\n","3           各位 質疑\n","4           那邊 一堆\n","           ...   \n","31865       蜜月 澎湖\n","31866       中天 新聞\n","31867       只記 這是\n","31868       有個 應該\n","31869    3000 消費券\n","Name: context, Length: 31870, dtype: object\n","df1['title'] = \n","0             什麼 10\n","1            請進 一首歌\n","2            唐吉軻 消息\n","3           網議 愛莉莎莎\n","4           不先去 改機車\n","            ...    \n","31865        濟州島 那裡\n","31866    Youtube 更敢\n","31867        哪一部 電影\n","31868         有個 和尚\n","31869        消費券 怎麼\n","Name: title, Length: 31870, dtype: object\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6UFMwarkEc4K"},"source":[""],"execution_count":null,"outputs":[]}]}