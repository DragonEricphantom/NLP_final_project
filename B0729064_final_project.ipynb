{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B0729064_final_project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.7.7 64-bit"},"language_info":{"name":"python","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","interpreter":{"hash":"9164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5"}},"cells":[{"cell_type":"code","metadata":{"id":"xKbMCo5kojju"},"source":["\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"deCs9KMmuwuJ"},"source":["\n","   \n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5W6h8nAnMdv1"},"source":["# 載入使用的套件\n","import pandas as pd\n","import requests\n","import json\n","import re\n","infos = []\n","# 撰寫簡單的函數，透過輸入文章ID，就輸出文章的資料\n","def Crawl(ID):\n","  info = {}\n","  link = 'https://www.dcard.tw/_api/posts/' + str(ID)\n","  requ = requests.get(link)\n","  rejs = json.loads(requ.text)\n","  for d in rejs:\n","    info['ID'] = d['id']\n","    info['title'] = d['title']\n","    info['excerpt'] = d['excerpt']\n","    info['forumName'] = d['forumName']\n","  infos.append(info)\n","\n","\n","\n","url = 'https://www.dcard.tw/_api/posts?limit=100'\n","resq = requests.get(url)\n","rejs = json.loads(resq.text)\n","for d in rejs:\n","  info = {}\n","  info['ID'] = d['id']\n","  info['title'] = d['title']\n","  info['excerpt'] = d['excerpt']\n","  info['forumName'] = d['forumName']\n","  infos.append(info)\n","\n","df = pd.DataFrame(infos)\n","df.columns = ['ID','title','excerpt','forumName']\n","print(df)\n","df.to_csv('dcard.csv', encoding='utf_8_sig')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d4d_XAXB2mU0"},"source":["# 載入使用的套件\n","import pandas as pd\n","import requests\n","import json\n","import re\n","infos = []\n","# 撰寫簡單的函數，透過輸入文章ID，就輸出文章的資料\n","def Crawl(ID):\n","  info = {}\n","  link = 'https://www.dcard.tw/_api/posts/' + str(ID)\n","  requ = requests.get(link)\n","  rejs = json.loads(requ.text)\n","  for d in rejs:\n","    info['ID'] = d['id']\n","    info['title'] = d['title']\n","    info['excerpt'] = d['excerpt']\n","    info['forumName'] = d['forumName']\n","  infos.append(info)\n","\n","\n","\n","url = 'https://www.dcard.tw/_api/posts?limit=100'\n","resq = requests.get(url)\n","rejs = json.loads(resq.text)\n","for d in rejs:\n","  info = {}\n","  info['ID'] = d['id']\n","  info['title'] = d['title']\n","  info['excerpt'] = d['excerpt']\n","  info['forumName'] = d['forumName']\n","  infos.append(info)\n","\n","df = pd.DataFrame(infos)\n","df.columns = ['ID','title','excerpt','forumName']\n","print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJoN5UQlzpFM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MFb55c14OQK","executionInfo":{"status":"ok","timestamp":1629722526847,"user_tz":-480,"elapsed":37564,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"7a611e94-7f5b-4dee-ad0a-77c9a30d7abd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"id":"Sy2IsyUUAEpM","executionInfo":{"status":"error","timestamp":1629710029055,"user_tz":-480,"elapsed":1140,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"fff8e401-d9f1-49ff-9ae5-b0b8b5f98537"},"source":["#hw2\n","import multiprocessing\n","from multiprocessing import Process, Queue\n","\n","import time\n","from lxml import etree\n","import requests\n","from bs4 import BeautifulSoup\n","\n","#url = 'https://movies.yahoo.com.tw/moviegenre_result.html?genre_id=1'\n","\n","infos = []\n","\n","\n","def check_req_url(url): #測試請求網址是否請求成功\n","    r = requests.get(url) #請求網址\n","    #print(resp.status_code) #錯誤時404,成功時200\n","    if r.status_code != 200:  #如果請求失敗\n","        print('Invalid url:', r.url) #印出請求失敗的網址\n","        return \"fail\" #回傳失敗提示訊息\n","    else:\n","        return r.text #回傳請求成功的html文字\n","\n","def get_news_info(webpage): #抓取電影資訊\n","  soup = BeautifulSoup(webpage) #網頁解析\n","  exist = soup.find_all('div', id = 'content_l')\n","  if exist:\n","    for d in soup.find_all('div', id=\"content_l\"):\n","        info = {}\n","        info['中文名稱'] = d.find('div', class_='movie_intro_info_r').find('h1').text.strip()\n","        info['英文名稱'] = d.find('div', class_='movie_intro_info_r').find('h3').text.strip()\n","        info['上映日期'] = d.find('span').text.strip()\n","        info['劇情介紹'] = d.find('div', class_='gray_infobox storeinfo').find('div', class_='gray_infobox_inner').find('span',id='story').text.strip()\n","        # info['連接介紹'] = d.find('div', class_='release_movie_name').find('a', class_='gabtn').get('href')\n","        # net = d.find('div', class_='release_movie_name').find('a', class_='gabtn').get('href')\n","        print(info['中文名稱'])\n","        if d.find('div', class_='movie_intro_info_r').find('div',class_='level_name'):\n","          info['oneLabel'] = d.find('div', class_='movie_intro_info_r').find('div',class_='level_name').a.text.strip()\n","        else:\n","          info['oneLabel'] = 'NA'\n","        # print(info['oneLabel'])\n","        level_name = d.find('div', class_='movie_intro_info_r').find_all('div',class_='level_name')\n","        str =''\n","        for level in level_name:\n","          str += level.a.text.strip()+',' \n","        info['Label'] = str\n","           \n","        infos.append(info) \n","    return infos\n","  else:\n","    return 0\n","\n","urlarray = []\n","if __name__ == '__main__':\n","  url = 'https://tw.news.yahoo.com/'\n","  r = requests.get(url)\n","  print(r)\n","  soup = BeautifulSoup(r.text) #網頁解析\n","  for d in soup.find('div', class_ = '_yb_14rjq _yb_vb2n7 '):\n","    print(\"in soup.find_all\")\n","    urltemp = {}   \n","    urltemp['list_url'] = d.find('div',class_='ybar-mod-navigation _yb_1bzrv    _yb_1p468 ybar-shift-more-menu ').find_all('li',class_='_yb_1fp89 _yb_12rh5').find.a.get('href')\n","    print(urltemp['list_url'])\n","    urltemp['list_name'] = d.find_all('li',class_='_yb_1fp89 _yb_12rh5').find('div',class_='_yb_1fibg').text.strip()\n","    print(\"urltemp['list_name'] = \")\n","    print(urltemp['list_name'])\n","    urlarray.append(urltemp)\n","\n","for i in urlarray:\n","  print(i)\n","\n","  # for i in urlarray:\n","  #   # 'https://movies.yahoo.com.tw/movieinfo_main/%d'%(i)+''\n","  #   url = i\n","  #   r = requests.get(url, headers=headers)\n","  #   soup = BeautifulSoup(r.text) #網頁解析\n","  #   if soup.find_all('div', class_='movie_intro_info_r'):\n","  #     infos = get_week_new_movies(r.text)\n","  #   else:\n","  #     print(\"NON\")\n","  #     continue\n","  # import pandas as pd\n","\n","  # df = pd.DataFrame(infos)\n","  # df.to_csv('movies.csv', encoding='utf_8_sig')     \n","      \n","  # df    \n","    "],"execution_count":23,"outputs":[{"output_type":"stream","text":["<Response [200]>\n","in soup.find_all\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-43505d9c9e22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"in soup.find_all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0murltemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0murltemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'list_url'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ybar-mod-navigation _yb_1bzrv    _yb_1p468 ybar-shift-more-menu '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_yb_1fp89 _yb_12rh5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'href'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murltemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'list_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0murltemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'list_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_yb_1fp89 _yb_12rh5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_yb_1fibg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"]}]},{"cell_type":"code","metadata":{"id":"_pVuRSHjAGPr"},"source":["#hw3\n","\n","import re\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","import jieba\n","import jieba.analyse\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split    #分割資料集\n","\n","def cutval(df):\n","    topcut = []\n","    for d in df['劇情介紹']:\n","        top = jieba.analyse.extract_tags(d,topK=2)\n","        topcut.append(top)\n","    typecut = []\n","    for d in df['oneLabel']:\n","        # ch =re.compile(\"[\\u4e00-\\u9fa5]\")\n","        # seg_word =  \"\".join(ch.findall(d))\n","        # top = jieba.lcut(seg_word)\n","        typecut.append(d)\n","    # print(typecut)\n","    namecut = []\n","    for d in df['中文名稱']:\n","        ch =re.compile(\"[\\u4e00-\\u9fa5]\")\n","        name =  \"\".join(ch.findall(d))\n","        # print(\"name = \")\n","        # print(name )\n","        namecut.append(name)\n","    # print(namecut)\n","    data = { 'type':typecut ,'name':namecut,'article':topcut}\n","    df1 = pd.DataFrame(data)\n","\n","    df1['type'] = df1['type'].apply(lambda x :  str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","    df1['article'] = df1['article'].apply(lambda x :  str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","    df1['key'] = df1['name'].astype(str)+' '+df1['type'].astype(str)+' '+df1['article'].astype(str)\n","    return df1\n","\n","def count(df):\n","    vectorizer = CountVectorizer()\n","    X = vectorizer.fit_transform(df['key'])\n","    tfidf = TfidfTransformer() \n","    tf=tfidf.fit_transform(X)\n","    word = vectorizer.get_feature_names() #詞袋 \n","    print(\"tf = \")\n","    print(tf)\n","    return tf\n","\n","def knn(X,Y):\n","    X_train, X_test, y_train, y_test = train_test_split(X, Y['type'].str[0:2], test_size=0.07)\n","    clf=KNeighborsClassifier(n_neighbors=51)\n","    clf.fit(X_train,y_train)\n","\n","    y_pred = clf.predict(X_test)    # 預測模型 \n","    y_test=y_test.values\n","    print(\"精準度：\",metrics.accuracy_score(y_test, y_pred))\n","\n","df = pd.read_csv('/content/drive/MyDrive/NatureLanguageProgram/B0729064_NLP_HW3/movies.csv')\n","df = df.drop(labels=['Unnamed: 0'],axis='columns')\n","df1 = cutval(df) \n","tf = count(df1)\n","knn(tf,df1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uii0T7ixLGXP","executionInfo":{"status":"ok","timestamp":1629708304102,"user_tz":-480,"elapsed":3385,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"10f436c4-0ea4-41a6-93e1-961bb61baaca"},"source":["\n","import requests\n","\n","from bs4 import BeautifulSoup\n","\n","from urllib.parse import quote\n","\n","\n","\n","###############################################################################\n","\n","#                       股票機器人 爬蟲範例 【html爬蟲】                      #\n","\n","###############################################################################\n","\n","\n","\n","def getYahooNews(keyword):\n","\n","    url = 'https://tw.finance.yahoo.com/news_search.html?ei=Big5&q=' + quote(keyword.encode('big5'))\n","\n","    #請求網站\n","\n","    list_req = requests.get(url)\n","\n","    #將整個網站的程式碼爬下來\n","\n","    soup = BeautifulSoup(list_req.content, \"html.parser\")\n","    getNew= soup.find('table',{'id':'newListTable'}) #抓到新聞的table\n","    getAllNews=getNew.find_all('table');\n","\n","    \n","\n","    for i in range(0,len(getAllNews)):\n","\n","        onebody=getAllNews[i];\n","\n","        NewTitle=onebody.find_all('a',{'class':'mbody'});\n","\n","        NewsDt=onebody.find('span',{'class':'t1'});\n","\n","        if (len(NewTitle) >= 1):\n","\n","            print(\"文章標題:\"+NewTitle[1].text+NewsDt.text)\n","\n","        Context=onebody.find_all('span',{'class':'mbody'});\n","\n","        if(len(Context)>=1):\n","\n","            print(\"內文:\"+Context[0].text)\n","\n","        print('');\n","\n","\n","\n","getYahooNews('中鋼')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["文章標題:【台股盤後】航運半導體雙主流領軍 台股強彈399.9點（中央社 2021-08-23）\n","內文:（中央社記者潘智義台北2021年8月23日電）美股反彈帶動台積電(2330)、聯電(2303)等半導體族群走揚，攜手散裝及貨櫃族群強漲，盤中大漲逾400點；指數終場大漲399.9點，以16741.84\n","\n","\n","文章標題:電子、航運領軍，台股突破性大漲，挑戰半年線（財訊快報 2021-08-23）\n","內文:【財訊快報／研究員鄭心丹】美股四大指數上周五收紅，道瓊上漲225.96點，Fed卡普蘭意外「微調」鷹派觀點，科技股走高，微軟、思科、Salesforce領軍大漲之下，那斯達克漲幅1.19%。亞股周一全\n","\n","\n","文章標題:豐興開出本週盤價，鋼筋、廢鋼及型鋼產品維持平盤（財訊快報 2021-08-23）\n","內文:...公噸價格2.26萬元；廢鋼每公噸價格在1.14萬元、型鋼每公噸2.7萬元。國際原料行情，美國大船廢鋼本週無報價，日本2H廢鋼本週上漲至480美元/噸，美國貨櫃廢鋼本週報價上漲至450美元/噸，澳\n","\n","\n","文章標題:【台股盤中】電子航運領軍 台股盤中站回16700點（中央社 2021-08-23）\n","內文:...570元，大漲3.26%。聯發科(2454)來到910元，上漲4.42%。大立光(3008)來到2810元，上漲2.36%。鴻海(2317)107元，上漲2.39%。貨櫃三雄長榮(2603)盤中\n","\n","\n","文章標題:《台北股市》盤中焦點股：台積電、新鋼、慧洋-KY、萬海、世芯-KY（時報資訊 2021-08-23）\n","內文:...上周五外資逆勢轉買超2953張，加上ADR漲1.67%，今早股價彈升近4%、躍過5日線。 2.台積電(2330)：外資賣超不停歇、上周五ADR跌2.17%，惟投信買超力挺，加上高盛證券送暖重申買\n","\n","\n","文章標題:【Y早報】微軟、思科領軍科技股 美四大指數收紅（Yahoo奇摩股市 2021-08-23）\n","內文:（開盤日09:00出刊）晶片備貨旺季，封測產能吃緊，3測試廠旺到明年；當沖降稅延長，牽動權證避險降稅 ，已釋出2版本；台股進入超跌區，救市資金偏愛16檔具3條件股。\n","\n","\n","文章標題:《鋼鐵股》新光鋼 今年EPS挑戰9元（時報資訊 2021-08-23）\n","內文:...工程需求榮景持續加持下，目前下半年仍然穩定接單，且加工製造訂單可望延續到2022上半年，希望能帶動業績跟著穩定成長。 對於經濟部剛公布的7月外銷訂單達到553億元、年成長21.4％，為連17個月\n","\n","\n","文章標題:《熱門族群》16檔官股券商救援 贏面大（時報資訊 2021-08-21）\n","內文:...接首選標的，也是意料中的事。官股操作偏防守而不拉抬，但其著墨較多的個股通常表現相對抗跌。 除台積電近二日獲官股券商買超45.76億元、高居名單之首外，包括陽明、長榮、國巨、友達、統一、萬海、聯電\n","\n","\n","文章標題:《產業》澳洲鐵礦砂大跌 中鋼：板材鋼品暫無下修壓力（時報資訊 2021-08-21）\n","內文:【時報-台北電】近來受到大陸環保限產與美元走強影響，澳洲鐵礦砂每公噸價跌破130美元，創近九個月來新低，累計三個多月來大跌百餘美元、跌幅高達44.7％，對板類鋼材帶來價格下修的壓力。 對此，中鋼認為，\n","\n","\n","文章標題:《貿易》反傾銷稅裁罰屆期 對陸兩鋼品啟動落日調查（時報資訊 2021-08-21）\n","內文:...預計最快為2022年5月完成傾銷調查認定，在調查期間將繼續課徵反傾銷稅。 若最後調查認定有傾銷情況，我國將延長課稅期間至2026年。若無傾銷情況，則兩大產品反傾銷稅則在調查完成後落日，財政部則退\n","\n","\n","\n"],"name":"stdout"}]}]}