{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B0729064_final_project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.7.7 64-bit"},"language_info":{"name":"python","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","interpreter":{"hash":"9164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5"}},"cells":[{"cell_type":"code","metadata":{"id":"BwLhd1KctYOZ","colab":{"base_uri":"https://localhost:8080/","height":230},"executionInfo":{"status":"error","timestamp":1629532804528,"user_tz":-480,"elapsed":293,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"b4a07903-5399-45b1-f973-41e63423926a"},"source":["\n","keyword='养发|沐发|沐头|洗发|涂之立生|润发|立生|脱发|长发|发落'\n","import numpy as np \n","import pandas as pd \n","import glob\n","import os\n","filelist=sorted(glob.glob(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/ChineseMedicalBook/*.txt\"))\n","\n","def agent(filelist):\n","  reader=pd.read_csv(filelist,header=None,sep='\\r',engine='python',error_bad_lines=False)\n","  reader.columns=['data']\n","  reader['book']=reader['data'][0]\n","  reader['item']=reader['data'].apply(lambda x: x if '<篇名>' in x else None  )\n","  reader['item']=reader['item'].fillna(method='ffill')\n","  reader['keyWords']=reader['data'].str.contains(keyword)\n","  temp=reader[reader['keyWords']==True]\n","  return temp[['data','item','book']]\n","\n","tempall=pd.DataFrame(agent(filelist[0]))\n","for i in range(681):\n","    temp=agent(filelist[i])\n","    tempall=tempall.append(temp,ignore_index=True,)   \n","tempall.to_csv('output1.csv',index=None) \n","tempall"],"execution_count":9,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-e508c05b2357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'book'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtempall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilelist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m681\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilelist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5f0wrKCIfl9","executionInfo":{"status":"ok","timestamp":1629473043196,"user_tz":-480,"elapsed":415,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"1fe8d92a-e832-4003-ee2d-610c9b809c02"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"TYYrEnZ0eV51"},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import glob\n","import os\n","from opencc import OpenCC\n","cc = OpenCC('s2t')\n","path = \"ChineseMedicalBook\"#資料夾目錄\n","files= os.listdir(path) #得到資料夾下的所有檔名稱\n","# print(type(files))\n","# print(files)\n","store = open('zh','w')\n","for file in files:\n","    file=open(file,'r',encoding='utf-8')\n","    txt = []\n","    for line in file.readlines():\n","        txt.append(line)\n","    for data in txt:\n","        data = cc.convert(data)\n","    store.write(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s-xVJmp-kq2c"},"source":["\n","import re\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","import jieba\n","import jieba.analyse\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split    #分割資料集\n","\n","def cutval(df):\n","    topcut = []\n","    for d in df['劇情介紹']:\n","        top = jieba.analyse.extract_tags(d,topK=2)\n","        topcut.append(top)\n","    typecut = []\n","    for d in df['oneLabel']:\n","        # ch =re.compile(\"[\\u4e00-\\u9fa5]\")\n","        # seg_word =  \"\".join(ch.findall(d))\n","        # top = jieba.lcut(seg_word)\n","        typecut.append(d)\n","    # print(typecut)\n","    namecut = []\n","    for d in df['中文名稱']:\n","        ch =re.compile(\"[\\u4e00-\\u9fa5]\")\n","        name =  \"\".join(ch.findall(d))\n","        # print(\"name = \")\n","        # print(name )\n","        namecut.append(name)\n","    # print(namecut)\n","    data = { 'type':typecut ,'name':namecut,'article':topcut}\n","    df1 = pd.DataFrame(data)\n","\n","    df1['type'] = df1['type'].apply(lambda x :  str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","    df1['article'] = df1['article'].apply(lambda x :  str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","    df1['key'] = df1['name'].astype(str)+' '+df1['type'].astype(str)+' '+df1['article'].astype(str)\n","    return df1\n","\n","def count(df):\n","    vectorizer = CountVectorizer()\n","    X = vectorizer.fit_transform(df['key'])\n","    tfidf = TfidfTransformer() \n","    tf=tfidf.fit_transform(X)\n","    word = vectorizer.get_feature_names() #詞袋 \n","    \n","    return tf\n","\n","def knn(X,Y):\n","    X_train, X_test, y_train, y_test = train_test_split(X, Y['type'].str[0:2], test_size=0.07)\n","    clf=KNeighborsClassifier(n_neighbors=51)\n","    clf.fit(X_train,y_train)\n","\n","    y_pred = clf.predict(X_test)    # 預測模型 \n","    y_test=y_test.values\n","\n","    print(\"精準度：\",metrics.accuracy_score(y_test, y_pred))\n","\n","df = pd.read_csv('movies.csv')\n","df = df.drop(labels=['Unnamed: 0'],axis='columns')\n","df1 = cutval(df) \n","tf = count(df1)\n","knn(tf,df1)"],"execution_count":null,"outputs":[]}]}