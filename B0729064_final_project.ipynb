{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B0729064_final_project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3.7.7 64-bit"},"language_info":{"name":"python","version":"3.7.7","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","interpreter":{"hash":"9164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MFb55c14OQK","executionInfo":{"status":"ok","timestamp":1629894415044,"user_tz":-480,"elapsed":41393,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"b863d24c-04c3-4514-b985-843a760e4d3f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_pVuRSHjAGPr","colab":{"base_uri":"https://localhost:8080/","height":549},"executionInfo":{"status":"ok","timestamp":1629895191994,"user_tz":-480,"elapsed":1604,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"ea7a7076-204a-49d7-d36d-4727f0a2cd89"},"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","from glob import glob\n","import jieba\n","import jieba.analyse\n","\n","import re\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split  \n","\n","def cuttrain_data(df):\n","  cuttitle = []\n","  for d in df['title']:\n","      cut = jieba.analyse.extract_tags(d,topK=2)\n","      cuttitle.append(cut)\n","  print(\"in cuttitle\")\n","  for d in cuttitle:\n","    print(d)\n","  cutcontext = []\n","  for d in df['context']:\n","      cut = jieba.analyse.extract_tags(d,topK=10)\n","      cutcontext.append(cut)\n","  print(\"in context\")\n","  for d in cutcontext:\n","    print(d)\n","  push = []\n","  for d in df['push']:\n","      push.append(d)\n","  down = []\n","  for d in df['down']:\n","      down.append(d)\n","\n","  data = { 'push':push ,'down':down ,'title':cuttitle,'context':cutcontext}\n","  df1 = pd.DataFrame(data)\n","\n","  # df1['push'] = df1['push'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\"))\n","\n","  # df1['down'] = df1['down'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\"))\n","  # print(\"df1['down'] = \")\n","  # print(df1['down'])\n","  # df1['context'] = df1['context'].apply(lambda x : str(x)[1:-2].replace(\"'\",\"\"))\n","  # print(\"df1['context'] = \")\n","  # print(df1['context'])\n","  # df1['title'] = df1['title'].apply((lambda x : str(x)[1:-2].replace(\"'\",\"\"))\n","  # # print(\"df1['title'] = \")\n","  # # print(df1['title'])\n","  # df1['ans_push'] = df1['title'].astype(str)+' '+df1['push'].astype(str)+' '+df1['context'].astype(str)\n","  # df1['ans_down'] = df1['title'].astype(str)+' '+df1['down'].astype(str)+' '+df1['context'].astype(str)\n","  return df1\n","\n","def cuttest_data(df):\n","  cuttitle = []\n","  for d in df['title']:\n","      cut = jieba.analyse.extract_tags(d,topK=2)\n","      cuttitle.append(cut)\n","  cutcontext = []\n","  for d in df['context']:\n","      cut = jieba.analyse.extract_tags(d,topK=10)\n","      cutcontext.append(cut)\n","\n","def count(df):\n","    vectorizer = CountVectorizer()\n","    X = vectorizer.fit_transform(df['ans_push'])\n","    tfidf = TfidfTransformer() \n","    tf=tfidf.fit_transform(X)\n","    word = vectorizer.get_feature_names() #詞袋 \n","    print(\"tf = \")\n","    print(tf)\n","    return tf\n","\n","def count(df):\n","    vectorizer = CountVectorizer()\n","    X = vectorizer.fit_transform(df['ans_down'])\n","    tfidf = TfidfTransformer() \n","    tf=tfidf.fit_transform(X)\n","    word = vectorizer.get_feature_names() #詞袋 \n","    print(\"tf = \")\n","    print(tf)\n","    return tf\n","\n","def knn(X,Y):\n","    X_train, X_test, y_train, y_test = train_test_split(X, Y['type'].str[0:2], test_size=0.07)\n","    clf=KNeighborsClassifier(n_neighbors=51)\n","    clf.fit(X_train,y_train)\n","\n","    y_pred = clf.predict(X_test)    # 預測模型 \n","    y_test=y_test.values\n","    print(\"精準度：\",metrics.accuracy_score(y_test, y_pred))\n","\n","\n","\n","\n","\n","train_data = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train_data.csv\")\n","train_label = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train_label.csv\")\n","test_data = pd.read_csv(\"/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/test_data.csv\")\n","files = glob('/content/drive/MyDrive/NatureLanguageProgram/NLP_final_project/train*.csv')\n","df_list = [pd.read_csv(file) for file in files]  #串列中包含兩個Pandas DataFrame\n","total_train_data = pd.merge(df_list[0], df_list[1], on='index')\n","\n","\n","the_training_data = total_train_data[['title' ,'context' ,'push' ,'down']]\n","the_training_data\n","for d in the_training_data:\n","  print(d)\n","  print(type(d))\n","the_test_data = test_data[['title' ,'context']]\n","the_test_data\n","\n","# ls = cuttrain_data(the_training_data)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["title\n","<class 'str'>\n","context\n","<class 'str'>\n","push\n","<class 'str'>\n","down\n","<class 'str'>\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>title</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>假設給你一億操作一年，接受的人多嗎</td>\n","      <td>\\n剛想到一個假設的情境\\n\\n有點像韓劇或日劇的感覺\\n\\n大家常說本多終勝\\n\\n假如今...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>台中朝馬房價1300萬算高嗎</td>\n","      <td>\\n大家安安\\n\\n最近看到一個文宣\\n台中朝馬站附近 帝寶1300萬\\n請吳宗憲女兒打廣告...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>當健身仔是不是很麻煩</td>\n","      <td>\\n其實也不是說多麻煩\\n\\n大概就是這個不行吃 那個不能吃\\n\\n肥仔在吃雞排、控肉便當\\...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>過35不穿西裝褲襯衫的484低端</td>\n","      <td>\\n到35歲之後\\n\\n就是大人\\n\\n出門就是正式\\n\\n就是個人行動名片\\n\\n不穿西裝...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>500百元卡在一百元的包包裡面要如何抉擇</td>\n","      <td>\\n前幾年\\n去泰國的時候\\n\\n在路邊攤隨手買了一個\\n三葉草的包包\\n雖然知道8成是假的...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31844</th>\n","      <td>如何猜到女孩兒的心</td>\n","      <td>\\n這樣的夜熱鬧的街\\n\\n問你想到了誰緊緊鎖眉\\n\\n我的喜悲　隨你而飛　\\n\\n擦了又濕...</td>\n","    </tr>\n","    <tr>\n","      <th>31845</th>\n","      <td>坂本龍一算是日本周杰倫嗎？</td>\n","      <td>\\n親，批踢踢的鄉民晚上好\\n\\n小魯在八卦板上看到《俘虜》(aka聖誕快樂，勞倫斯先生)之...</td>\n","    </tr>\n","    <tr>\n","      <th>31846</th>\n","      <td>為何日本公立高中不強調升學考大學??</td>\n","      <td>\\n之前有位在日本居住的親戚討論小孩升學問題，\\n該親戚(我應該要叫舅舅)說日本公立高中不強...</td>\n","    </tr>\n","    <tr>\n","      <th>31847</th>\n","      <td>明天要去士東市場 有什麼必吃嗎？</td>\n","      <td>\\n久聞遠在天母的士東市場有許多非常好吃的東西\\n比水源市場 南門市場還厲害\\n想請教台北的...</td>\n","    </tr>\n","    <tr>\n","      <th>31848</th>\n","      <td>我是你的索隆是什麼意思</td>\n","      <td>\\n我是你的索隆\\n這是什麼意思\\n是暗示蕭敬騰也會三刀流嗎\\n\\n還是航海王要完結了？\\n</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31849 rows × 2 columns</p>\n","</div>"],"text/plain":["                       title                                            context\n","0          假設給你一億操作一年，接受的人多嗎  \\n剛想到一個假設的情境\\n\\n有點像韓劇或日劇的感覺\\n\\n大家常說本多終勝\\n\\n假如今...\n","1             台中朝馬房價1300萬算高嗎  \\n大家安安\\n\\n最近看到一個文宣\\n台中朝馬站附近 帝寶1300萬\\n請吳宗憲女兒打廣告...\n","2                 當健身仔是不是很麻煩  \\n其實也不是說多麻煩\\n\\n大概就是這個不行吃 那個不能吃\\n\\n肥仔在吃雞排、控肉便當\\...\n","3           過35不穿西裝褲襯衫的484低端  \\n到35歲之後\\n\\n就是大人\\n\\n出門就是正式\\n\\n就是個人行動名片\\n\\n不穿西裝...\n","4       500百元卡在一百元的包包裡面要如何抉擇  \\n前幾年\\n去泰國的時候\\n\\n在路邊攤隨手買了一個\\n三葉草的包包\\n雖然知道8成是假的...\n","...                      ...                                                ...\n","31844              如何猜到女孩兒的心  \\n這樣的夜熱鬧的街\\n\\n問你想到了誰緊緊鎖眉\\n\\n我的喜悲　隨你而飛　\\n\\n擦了又濕...\n","31845          坂本龍一算是日本周杰倫嗎？  \\n親，批踢踢的鄉民晚上好\\n\\n小魯在八卦板上看到《俘虜》(aka聖誕快樂，勞倫斯先生)之...\n","31846     為何日本公立高中不強調升學考大學??  \\n之前有位在日本居住的親戚討論小孩升學問題，\\n該親戚(我應該要叫舅舅)說日本公立高中不強...\n","31847       明天要去士東市場 有什麼必吃嗎？  \\n久聞遠在天母的士東市場有許多非常好吃的東西\\n比水源市場 南門市場還厲害\\n想請教台北的...\n","31848            我是你的索隆是什麼意思     \\n我是你的索隆\\n這是什麼意思\\n是暗示蕭敬騰也會三刀流嗎\\n\\n還是航海王要完結了？\\n\n","\n","[31849 rows x 2 columns]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"6UFMwarkEc4K"},"source":[""],"execution_count":null,"outputs":[]}]}